<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interpretability</title>
    <link rel="stylesheet" href="/styles.css">
    <script src="/include.js"></script>

    <!-- MathJax Configuration -->
    <script>
    MathJax = {
        tex: {
          inlineMath: [['\\(', '\\)']],
          displayMath: [['$', '$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div data-include="/header.html"></div>

    <main class="main-content">
        <div class="content-article">
            <h1 id="a-biased-overview-of-llm-interpretability">A biased
            overview of LLM interpretability</h1>
            <p>Created: August 14, 2025 7:00 AM</p>
            <p>I’ve long avoided telling friends about my interest in
            LLM interpretability because I couldn’t find a satisfactory
            way to explain it. The usual introductions stay
            frustratingly vague—“opening the black box,” “understanding
            the inner workings”—while more technical discussions often
            jump straight into methodological details. What’s missing is
            something between these extremes, something that expresses
            the vague goal of interpretability in more concrete terms
            that directly connects to methodologies and results.</p>
            <p>This article attempts to fill that gap. As the title
            suggests, it’s highly biased. I will focus exclusively on
            what I find most compelling (roughly speaking, mechanistic
            interpretability of LLMs). Framing everything through my own
            lens, I may be reinventing wheels or taking research out of
            context. But this biased perspective is perhaps not all bad.
            Scientific fields—especially young fields like
            interpretability—progress and define themselves through
            merging a diversity of biased perspectives.</p>
            <h2 id="what-do-we-want-why-is-it-hard">What do we want? Why
            is it hard?</h2>
            <p>In my view, LLM interpretability research is
            fundamentally about the following question: <strong>Given a
            model, how do we find a good representation of it that
            clearly show what it knows and how it uses that
            knowledge?</strong></p>
            <h3 id="why-representation-matters">Why representation
            matters</h3>
            <p>Think about understanding a tree. You could describe it
            as a bunch of atoms and molecules. Or as collections of
            cells. Or as organs like leaves, branches, and trunk. Or
            just as “a tree.” All of these are valid ways to represent
            the same system, but each is useful for different purposes.
            When you’re pruning, thinking about branches makes sense.
            When studying photosynthesis, you need to think about cells
            and molecules.</p>
            <p>The power lies in the freedom to choose the right
            representation. A good representation highlights relevant
            information while hiding unnecessary complexity.</p>
            <h3 id="what-makes-a-good-representation">What makes a good
            representation</h3>
            <p>For LLMs, we’re stuck with two unhelpful extremes:
            examining trillions of individual parameters and activations
            (too detailed) or treating the whole model as a black box
            (too vague). To gain more insight on what the model knows
            and how it “thinks”, we need a representation where the
            model consists of elements that individually make sense to
            human.</p>
            <p>More concretely, this means finding mathematical
            functions that map the model’s parameters and activations to
            a new set of variables, where each variable represents some
            interpretable concept (knowledge, rules, etc.) and the value
            of a variable represents whether the corresponding concept
            is invoked when the model decides the next output token.</p>
            <h3 id="the-challenge">The challenge</h3>
            <p>The challenge in finding a good representation for LLMs
            first comes from defining a good representation.
            “Interpretable” and “making sense to human” are subjective
            concepts, making it hard to define mathematically what
            constitutes a good representation.</p>
            <p>A further challenge lies in finding new representations.
            With trees, you can naturally find more granular
            representations by decomposing large objects into smaller
            pieces or find more abstract representations by grouping
            similar or nearby components. This works because of physics:
            what makes up the tree mostly interact with its neighbors. A
            leaf doesn’t care about another leaf on another branch.</p>
            <p>LLMs don’t work this way. Sure, there are some modular
            pieces at the highest level (attention layers, feed-forward
            blocks), but within those pieces everything is densely
            connected to everything else. Information flows through
            complex webs that you can’t just cut apart cleanly.
            Everything influences everything else. There is no obvious
            way to generate a reasonable new representation based on
            what we already have.</p>
            <p>(The same problem exists in other complex systems with
            global coupling, like plasma physics or gravitational
            systems with many bodies. That’s one of the reasons why I
            did research on these topics before and do LLM
            interpretability research now.)</p>
            <h2
            id="an-emerging-paradigm-from-sparsity-to-interpretable-features">An
            emerging paradigm: from sparsity to interpretable
            features</h2>
            <h3
            id="sparsity-as-a-prerequisite-of-interpretability">Sparsity
            as a prerequisite of interpretability</h3>
            <p>While a precise definition of “interpretable” remains
            elusive, there is at least one prerequisite of
            interpretability that can be expressed mathematically:
            interpretable representations needs to be sparse.</p>
            <p>This follows from a simple fact: LLMs know too much. If
            interpretability required simultaneously understanding
            everything an LLM knows, it would be impossible. But we
            don’t need to understand everything at once. At a minimum,
            we only need to understand the tiny subset of the model’s
            knowledge that is relevant to the current conversation. In
            other words, we need to isolate currently relevant
            information from the model’s complete knowledge base.</p>
            <p>This introduces the requirement of sparsity. In an
            interpretable representation, the elements of this
            representation need to activate sparsely; you shouldn’t need
            all the elements to understand a particular
            conversation.</p>
            <h3 id="extracting-sparse-features">Extracting sparse
            features</h3>
            <p>Requiring sparsity provides a concrete starting point. We
            can now search for functions that map model parameters and
            activations to sparsely activating elements.</p>
            <p>How do we find such functions? Mostly by heuristics.
            First, activations is likely a better choice than parameters
            since they contain information about specific inputs and
            outputs. For transformers, we may narrow our focus further
            to residual stream activations—the internal workspace where
            each transformer block reads and writes information. (Other
            locations, such as the activations passed from the attention
            block to the feed-forward block, have also been used in
            studies.)</p>
            <p>Now comes a leap of faith: we assume these activations
            are sparse linear combinations of some “feature” vectors.
            This is certainly not guaranteed to work, but it is a
            somewhat natural assumption because of its simplicity and
            its alignment with the largely linear architecture of
            transformers. If this linear assumption is true, we can
            identify features using tools like sparse autoencoders and
            cross-layer transcoders. This transforms finding a sparse
            representation into a well-defined math problem. (There are
            many interesting math problems surrounding this subject, but
            I won’t go into this in more detail.)</p>
            <h3
            id="the-success-of-feature-based-methods-interpretability-comes-for-free">The
            success of feature-based methods: interpretability comes for
            free</h3>
            <p>This feature-based methodology has produced impressive
            results. To draw a few examples from Anthropic’s recent
            studies:</p>
            <ul>
            <li>Extracted features are often naturally interpretable.
            Inspections of when features are activated and how they
            affect the output show features with clear, unique meanings
            that range from concrete concepts (famous people, places) to
            abstract ones (code errors, sycophantic praise ← this one is
            particularly fun).</li>
            <li>Manipulating feature activations steers model behavior
            predictably, including “personality” changes.</li>
            <li>For short questions, entire reasoning processes can be
            mapped out in terms of which features are activated and how
            they promote/suppress each other’s activation. The “thought
            processes” revealed in these feature “circuits” reveal
            logical thinking similar to us and the ability to plan
            ahead—deciding what to write later, then figuring out how to
            get there. (Interestingly, the ability to plan ahead also
            means that the current autoregressive architecture may not
            be the most efficient choice, as the same plan-ahead will be
            executed multiple times.)</li>
            </ul>
            <p>It is worth noting that these features are extracted
            requiring only sparsity; interpretability seems to come for
            free. But why? In the following sections I will try to
            provide some intuition on why this works and highlight some
            complications and challenges.</p>
            <h3 id="why-are-sparse-features-naturally-interpretable">Why
            are sparse features naturally interpretable?</h3>
            <p>I don’t know if any good explanation exist in the
            literature, but my best guess is that it reflects a deeper
            parallel between LLMs and human cognition.</p>
            <p>Like LLMs, humans know too much. You don’t want to recall
            everything from high school when checking today’s weather.
            For efficient operation, our minds must organize knowledge
            into concepts that activate sparsely during thought
            processes.</p>
            <p>This creates a parallel: similar information (real-world
            data/training material) gets compressed in complex neural
            networks (brains/models) and represented as sparse concepts
            (mental concepts/model features). Both systems likely do
            this near-optimally, thanks to evolution (for us) and
            careful training (for LLMs). This creates a situation that
            resembles convergent evolution in biology: different species
            optimizing for similar tasks independently reach similar
            solutions. It is therefore not too surprising if the
            sparsest representation of a LLM happens to align quite well
            with the sparse concepts we extracted from our
            knowledge.</p>
            <p>This story comes with several caveats. There are major
            uncertainties on whether the underlying knowledge in
            training data sufficiently resembles human knowledge, and
            whether there is a single optimal way to store knowledge in
            neural networks regardless of the physical architecture of
            the network. Given these uncertainties, my attempted
            explanation might better be considered as a hypothesis.</p>
            <h3
            id="superposition-and-the-origin-of-model-efficiency">Superposition
            and the origin of model efficiency</h3>
            <p>One interesting fact I omitted is that the extracted
            features often outnumber the dimensions of the activations
            they come from—a phenomenon called “superposition.” This
            should alarm anyone familiar with linear algebra, as it
            means that features form an overcomplete basis and the
            decomposition of activations into feature amplitudes is not
            unique.</p>
            <p>Sparsity resolves this by providing additional
            constraints. (An informative prior, in more Bayesian terms.)
            We can resolve the degeneracy by selecting the sparsest
            feature combination (minimizing L1 norm). This allows a n-D
            activation vector to encode &gt;&gt;n features without
            breaking information theory.</p>
            <p>High-dimensional geometry also helps: exponentially many
            near-orthogonal vectors exist in high-dimensional spaces, so
            simply projecting the activation vector into features
            already retrieves feature amplitudes with little noise when
            features activate sparsely.</p>
            <p>Overall, superposition can be seen as a way to very
            efficiently compress information with little (but nozero)
            noise. This makes it crucial for LLM efficiency. If we force
            a model to not have any superposition, the number of
            features it can remember can be exponentially less.
            Understanding superposition’s efficiency—how many features
            (or connections between features) can be encoded while
            maintaining error tolerance—could be key to understanding
            performance and scaling laws.</p>
            <p>This also provides another way to conceptualize
            interpretability. Superposition causes information to be
            efficiently compressed but also too tangled to be
            interpretable, and we want to undo this compression. (As a
            result, an interpretable representation is likely larger
            than the original model, and this poses an engineering
            challenge.)</p>
            <h3 id="a-comeback-of-symbolism">A comeback of
            symbolism</h3>
            <p>In the early days of machine learning there was a
            competition between symbolism and connectionism. Symbolic
            models use rules and symbols to represent knowledge and
            perform logical reasoning, while connectionist models
            feature a black box of interconnected (artificial) neurons.
            The current majority opinion is that connectionism has
            clearly won and symbolism has become obsolete.</p>
            <p>However, the features we find in models essentially
            “translate” connectionist LLMs into symbolic models. This
            allows us to inspect the defeat of symbolism through a new
            lens: One may in fact achieve intelligence through a
            symbolic model, but the main bottleneck is the size of the
            model. The number of symbols required is so large that they
            are best stored in a highly compressed state in a neural
            network.</p>
            <h3 id="caveats-and-challenges">Caveats and challenges</h3>
            <p>The initial success of finding features in models is
            quite encouraging, but these results also reveal a number of
            caveats and challenges.</p>
            <ul>
            <li>Not all features are interpretable and monosemantic. Is
            this a generic property of LLMs or a limitation of current
            methodologies? Can we extract enough interpretable features
            to fully explain model behavior? On a related note, can we
            extrapolate the current methodologies to reduce the whole
            network to some symbolic network that contains more
            information on how features affect each other?</li>
            <li>Interpreting features requires somewhat subjective
            inspection of activation patterns and model effects. How do
            we automate this process, and establish objective criteria
            for the quality of interpretation while avoiding
            confirmation bias?</li>
            <li>How to overcome computation bottlenecks for extracting
            features from larger models or mapping reasoning processes
            for longer contexts? What if feature-based reasoning maps
            are too large to be reviewed by a human?</li>
            </ul>
            <p>Despite the challenge they pose, being able to phrase
            these well-defined problems and transforming the vague
            problem of interpretability into concrete engineering
            problems and mathematical puzzles is in itself a significant
            progress.</p>
            <h2 id="the-future">The future</h2>
            <p>Despite remaining challenges, I think it is reasonable to
            say that we are progressing rapidly in understanding how
            LLMs store and use information, and fully interpretable
            representations of production-grade LLMs may be on the
            horizon. This has implications for many applications.</p>
            <h3 id="auditing-and-steering-models-for-safety">Auditing
            and steering models for safety</h3>
            <p>Most of existing AI safety research treats models as
            black boxes and only probe or modify them externally. This
            approach has a fundamental flaw: black-box models can only
            be characterized by sampling behavior on different inputs,
            but the input parameter space is so vast and complex that
            comprehensive sampling is impossible. We can never
            confidently claim a model can’t do something harmful across
            all possible inputs.</p>
            <p>Feature-based interpretability could overcome this
            limitation by providing complete model representations. In
            theory, we could examine all features to determine if a
            model has harmful capabilities. This is expensive, but still
            far cheaper than sampling all possible inputs.</p>
            <p>Challenges remain: determining whether features
            contribute to unsafe behavior and identifying emergent
            behaviors from multi-feature interactions. But feature-based
            interpretability currently seems like the only path that
            might ultimately work for comprehensive safety auditing.</p>
            <p>This approach also enables precise, reliable, and
            inexpensive correction of known failure modes by pruning
            associated features or their connections.</p>
            <h3 id="making-better-models">Making better models</h3>
            <p>Beyond safety, predictable feature-based steering has
            many applications. We could tune model “character” for
            specific tasks—making coding agents more reflective and
            accuracy-focused, for example. More cynically, this might
            enable advertisement insertion in chatbots. (That might be
            the one good reason against interpretability research.)</p>
            <p>Interpretability may also illuminate how model behavior
            is constrained by factors like compute and parameter count.
            Right now, our best knowledge on this subject comes from
            empirical scaling laws, but we don’t really understand
            whether they come from. Feature-based interpretability gives
            us a better handle on that. We know that the model is a
            noisy compression of features, and the large number of
            interconnected features is what enables the intelligent
            behavior of models. With that, we can decompose scaling
            problem into three parts: (1) how performance scales with
            the number of features (or which features are learned); (2)
            how the maximum number of features scales with model
            parameters and model architecture; and (3) how features are
            picked up during training.</p>
            <p>The first question still can only be answered in an
            empirical manner, and the last question seems extremely
            challenging to answer quantitatively. But the second
            question, the maximum number of features (or largest feature
            network) that can be compressed in the model, is mostly a
            well-posed math problem. (It can be converted to questions
            such as how one use the outer product of low-rank matrices
            to approximate a sparse high-rank matrix.) Solving those can
            bring us a big step closer to understanding the scaling laws
            and optimize model architecture and training procedure
            accordingly.</p>
            <p>Knowing exactly how knowledge fits into models and how
            it’s used during inference could also enable better model
            modification. For example, one may extract features from
            large models, pruning irrelevant features, and compressing
            them into smaller networks for specialized applications.</p>
            <h3 id="understanding-the-human-mind">Understanding the
            human mind</h3>
            <p>Studies have established interesting parallels between
            LLM and human intelligence: LLMs often think through
            problems logically like humans rather than using completely
            foreign approaches. Given this parallel, one may use LLMs to
            better understand human minds. The main advantage of this
            approach is that artificial neural networks are much more
            convenient to work with than human neural networks. Pruning
            a few features in a model is so much easier (and way more
            ethical) than plucking out a few neurons from a human brain.
            This might enable a whole range of breakthroughs from
            studying psychopathology and neurodivergence to
            understanding what makes us human. (Is there anything in us
            that’s not in a monkey or a LLM? My guess is no.)</p>
            <h2 id="parting-thoughts">Parting thoughts</h2>
            <p>After spending a lot of my between-jobs vacation time on
            this article, I’m no longer sure whether it will ever be
            useful to anyone. Interpretability has moved incredibly fast
            over the past couple of years, and it’s so hard to piece
            together a full picture. It’s also quite possible that
            everything in this article will soon become irrelevant as we
            switch to completely different paradigms.</p>
            <p>But one thing is certain: we are in an exciting time for
            interpretability research. I’m eager to see—and hopefully
            contribute to—what comes next.</p>
        </div>
    
        <nav class="toc">
            <ul>
                <li><a href="#what-do-we-want-why-is-it-hard">What do we want? Why is it hard?</a>
                <ul>
                    <li><a href="#why-representation-matters">Why representation matters</a></li>
                    <li><a href="#what-makes-a-good-representation">What makes a good representation</a></li>
                    <li><a href="#the-challenge">The challenge</a></li>
                </ul>
                </li>
                <li><a href="#an-emerging-paradigm-from-sparsity-to-interpretable-features">An emerging paradigm: from sparsity to interpretable features</a>
                <ul>
                    <li><a href="#sparsity-as-a-prerequisite-of-interpretability">Sparsity as a prerequisite of interpretability</a></li>
                    <li><a href="#extracting-sparse-features">Extracting sparse features</a></li>
                    <li><a href="#the-success-of-feature-based-methods-interpretability-comes-for-free">The success of feature-based methods: interpretability comes for free</a></li>
                    <li><a href="#why-are-sparse-features-naturally-interpretable">Why are sparse features naturally interpretable?</a></li>
                    <li><a href="#superposition-and-the-origin-of-model-efficiency">Superposition and the origin of model efficiency</a></li>
                    <li><a href="#a-comeback-of-symbolism">A comeback of symbolism</a></li>
                    <li><a href="#caveats-and-challenges">Caveats and challenges</a></li>
                </ul>
                </li>
                <li><a href="#the-future">The future</a>
                <ul>
                    <li><a href="#auditing-and-steering-models-for-safety">Auditing and steering models for safety</a></li>
                    <li><a href="#making-better-models">Making better models</a></li>
                    <li><a href="#understanding-the-human-mind">Understanding the human mind</a></li>
                </ul>
                </li>
                <li><a href="#parting-thoughts">Parting thoughts</a></li>
            </ul>
        </nav>
    </main>

    <div data-include="/footer.html"></div>
</body>
</html>