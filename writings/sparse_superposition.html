<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Superposition of sparse features</title>
    <link rel="stylesheet" href="/styles.css">
    <script src="/include.js"></script>

    <!-- MathJax Configuration -->
    <script>
    MathJax = {
        tex: {
          inlineMath: [['\\(', '\\)']],
          displayMath: [['$', '$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div data-include="/header.html"></div>

    <main class="main-content">
        <div class="content-article">
            <h1
            id="superposition-of-sparse-features-exponential-embedding-efficiency-through-hypersphere-packing">Superposition
            of sparse features: Exponential embedding efficiency through
            hypersphere packing</h1>
            <p>Created: May 14, 2025 5:09 PM</p>
            <h1 id="summary">Summary</h1>
            <p>In this note, I use a toy model to investigate feature
            superposition in the limit of large feature sets and high
            sparsity. The main goal is to understand the efficiency and
            geometry of superposition, especially their asymptotic
            behaviors as model size increases. The main findings
            include:</p>
            <ul>
            <li>For sparse features, models tend to embed all features
            even when there are orders of magnitude more features than
            the model dimension.</li>
            <li>In a n-D model, features form a near-uniform lattice on
            the surface of a n-D hypersphere. Feature geometry can be
            quantitatively estimated using <a
            href="https://mathworld.wolfram.com/HyperspherePacking.html">hypersphere
            packing</a> theory.</li>
            <li>Superposition allows models to pack more features at the
            cost of ignoring weak input. Interference between nearby
            features is exponentially suppressed in large models.</li>
            <li>At given error tolerance, the number of features that
            can be embedded increases exponentially with model
            dimension. (Similar results have been obtained in previous
            studies on compressed sensing - e.g., <a
            href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611973075.95">Ba
            et al. 2010</a>)</li>
            <li>As the number of feature increases, low-dimensional
            models demonstrate a phase transition from non-degenerate
            embedding to degenerate embedding. Degeneracy can be
            suppressed by varying the loss function.</li>
            <li>Under the assumption of approximately symmetric
            embedding, the optimal embedding for high-dimensional (<span
            class="math inline">\(n\gtrsim 5\)</span>) models always
            embed all features and always avoid degeneracy.</li>
            </ul>
            <p>Results in this note can be reproduced using <a
            href="https://colab.research.google.com/drive/1161Cw2_sPQwVXTCZoOmjhci3FJd6sTNq?usp=sharing">this
            Colab notebook</a>.</p>
            <h1 id="motivation">Motivation</h1>
            <p>Superposition is a fundamental feature of neural
            networks. When the input is sparse (i.e., only a small
            fraction of features are nonzero for each input), a
            low-dimensional model can noisily encode more features than
            its dimensions. One important observation in the <a
            href="https://transformer-circuits.pub/2022/toy_model/index.html#learning">Toy
            Model paper</a> is that superposition becomes stronger as
            sparsity increases. Within the range of sparsity surveyed in
            the paper (up to <span class="math inline">\(1-S\sim
            10^{-2}\)</span> for several <span
            class="math inline">\(10^2\)</span> features), it is unclear
            whether the behavior converges.</p>
            <p>The motivation of this note is to study the asymptotic
            behavior of superposition in the limits of sparse input,
            large feature sets, and large models. Specifically, I will
            focus on two topics:</p>
            <ul>
            <li>The efficiency of superposition: How many features can a
            n-D model encode? What determines the tradeoff between
            embedding more features and accurately representing each
            embedded feature? This is important for understanding the
            scaling laws of large models and estimating their
            performance.</li>
            <li>The geometry of superposition: What kind of geometric
            structure do features form in superposition? (This refers to
            the geometry of embedded features in the space spanned by
            the model’s hidden dimensions.) This is important for
            feature extraction (e.g., can we extract features from only
            a fraction of the network’s dimensions?), for understanding
            the embedding of related features, and for understanding how
            computation takes place in the hidden layer.</li>
            </ul>
            <p>To investigate these problems, I study superposition in
            the limit of large <span class="math inline">\(n_{\rm
            feature}\)</span> and high sparsity. I will first develop a
            toy model and obtain some numerical results at low model
            dimensions (<span class="math inline">\(n\leq 8\)</span>). I
            will then use the insights from these models to motivate a
            theoretical framework for sparse feature embedding and
            obtain some asymptotic results for large models.</p>
            <h1 id="toy-model-setup">Toy model setup</h1>
            <p>Here we consider the same ReLU toy model used in the Toy
            Model paper, which involves encoding a vector <span
            class="math inline">\(x\in R^{n_{\rm features}}\)</span>
            into a lower-dimension vector <span
            class="math inline">\(h\in R^n\)</span> and recovering
            it:</p>
            <p><span class="math display">\[
            x&#39;={\rm ReLU}(W^TWx+b).
            \]</span></p>
            <p>For simplicity, I only consider the case when all feature
            dimensions are independent and equally important. In the Toy
            Model paper, to represent sparse features, each dimension of
            the input has a probability S of being zero; when it is
            nonzero, its value is uniformly distributed in [0,1].</p>
            <p>For this study, I consider the limit of infinitely sparse
            features with <span class="math inline">\((1-S)n_{\rm
            feature}\ll 1\)</span>. This limit is of course not
            realistic; for real-world applications one should at least
            expect <span class="math inline">\((1-S)n_{\rm
            feature}\gtrsim 1\)</span>. But this could be a useful
            limiting case to study before we venture towards more
            realistic parameters. In this limit, the problem is
            equivalent to optimizing the model for inputs where each
            input has exactly one nonzero dimension under the constraint
            of <span class="math inline">\(b_i&lt;0~\forall i\)</span>.
            In practice, this is how I train the model and define the
            loss function.</p>
            <p>The loss function is just the L2 loss,</p>
            <p><span class="math display">\[
            l= \sum_{i}^{}(x_i&#39;-x_i)^2.
            \]</span></p>
            <p>I will also experiment with different loss functions
            later in this note.</p>
            <p>To survey the different outcomes in the limit of large
            <span class="math inline">\(n_{\rm feature}\)</span> (more
            specifically, large <span class="math inline">\(n_{\rm
            feature}/n\)</span>), I train the model for <span
            class="math inline">\(n_{\rm feature}\)</span> between 4 and
            1024 and <span class="math inline">\(n\in
            \{2,3,4,6,8\}.\)</span></p>
            <h1 id="results">Results</h1>
            <h2
            id="models-tend-to-embed-all-features-sometimes-with-degeneracy">Models
            tend to embed all features, sometimes with degeneracy</h2>
            <p>One original motivation of this work is to study how many
            features a model can embed before it decides to ignore
            additional features. To my surprise, across all parameters I
            surveyed (up to 1024 features), the model always tries to
            embed all (or nearly all) of them.</p>
            <p>[A technical note: Here a feature is “embedded” means
            that input in this feature dimension can generate nontrivial
            output. Mathematically, a feature <span
            class="math inline">\(i\)</span> is embedded if <span
            class="math inline">\(|W_i|^2+b_i&gt;0\)</span>. Note that
            for our sparse models, the bias <span
            class="math inline">\(b_i\)</span> is always ≤ 0.]</p>
            <p>To understand how this takes place, let us first focus on
            the 2D models. The figure below visualizes the weights and
            responses of the models. In the top panels, each marker
            corresponds to the weight vector <span
            class="math inline">\(W_i\)</span> of a feature; within the
            same panel, all weight vectors have the same opacity, and
            degeneracy results in darker colors. In bottom panels, we
            plot <span class="math inline">\(x_i\)</span> vs. <span
            class="math inline">\(x_i&#39;\)</span> when the input is
            nonzero in feature <span
            class="math inline">\(i\)</span>.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/superposition_2d.png"
            alt="superposition_2d.png" style="width: 800px"/>
            <figcaption
            aria-hidden="true">superposition_2d.png</figcaption>
            </figure>
            <p>Below I make a similar plot for 3D models. The only
            difference is that the top panels now show the directions of
            weight vectors for embedded features projected to 2D, and we
            only show features whose weight is positive in the third
            dimension. This can be understood as projecting the
            directions of weight vectors onto the unit sphere and taking
            a front view.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/superposition_3d.png"
            alt="superposition_3d.png" style="width: 800px"/>
            <figcaption
            aria-hidden="true">superposition_3d.png</figcaption>
            </figure>
            <p>These low-dimensional models exhibit a few interesting
            trends:</p>
            <ul>
            <li>As the number of features increases, the model first
            tries to embed the features non-degenerately as additional
            directions, but eventually begins to embed new features
            degenerately.</li>
            <li>The directions of the embedded features form a nearly
            uniform lattice.</li>
            <li>As the model becomes more degenerate, the response of
            the model becomes weaker.</li>
            </ul>
            <p>Below I will discuss these trends, especially whether
            they persist in higher dimensions, using more detailed
            diagnostics.</p>
            <p>[A note on the 2D model at <span
            class="math inline">\(n_{\rm feature}=128\)</span>: This
            model is different from nearby models in the group, because
            the model settles into a sub-optimal local minimum. Later in
            this note I will show that the gain (the improvement of loss
            function compared to not embedding any feature) of this
            model is lower than other models in this group.]</p>
            <h2
            id="a-phase-transition-from-non-degenerate-to-degenerate-packing">A
            phase transition from non-degenerate to degenerate
            packing</h2>
            <p>Now let’s more systematically analyze the feature packing
            and the emergence of degeneracy. In the figure below, I plot
            the distribution of the angular separation between each
            embedded weight vector and its nearest neighbor, <span
            class="math inline">\(\theta_{\rm min}\)</span>; a
            separation <span class="math inline">\(\theta_{\rm
            min}=0\)</span> corresponds to degeneracy.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/theta_min.png"
            alt="theta_min.png" style="width: 400px"/>
            <figcaption aria-hidden="true">theta_min.png</figcaption>
            </figure>
            <p>For all models, increasing <span
            class="math inline">\(n_{\rm feature}\)</span> first causes
            the model to pack more features without degeneracy while
            decreasing <span class="math inline">\(\theta_{\rm
            min}\)</span>. For <span
            class="math inline">\(n=2,3,4\)</span>, there is eventually
            a phase transition into degenerate packing. Later on I will
            discuss whether this phase transition also exists for models
            with higher dimensions.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/multiplicity.png"
            alt="multiplicity.png" style="width: 800px"/>
            <figcaption aria-hidden="true">multiplicity.png</figcaption>
            </figure>
            <p>The above plots show how the number of directions and the
            average multiplicity of degenerate directions evolve for 3D
            and 4D models. The 3D model clearly demonstrate the phase
            transition; the 4D model is overall similar, but we do not
            have a clear view into the degenerate regime since I only
            surveyed <span class="math inline">\(n_{\rm
            feature}\)</span> up to 1024.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/amplitudes.png"
            alt="amplitudes.png" style="width: 800px"/>
            <figcaption aria-hidden="true">amplitudes.png</figcaption>
            </figure>
            <p>The above figure summarizes the amplitudes of weight
            vectors in the 3D and 4D models. Two trends are worth
            nothing: First, in the non-degenerate regime, weight vectors
            in the same model have nearly uniform amplitudes, which
            increases with <span class="math inline">\(n_{\rm
            feature}\)</span>. Second, in the degenerate regime, each
            set of degenerate weight vectors tend to have the same
            amplitude, and it negatively correlates with
            multiplicity.</p>
            <p>Interestingly, for 2D models the degenerate weight
            vectors show a different amplitude distribution where one
            vector tends to have large amplitude and the remaining ones
            share a uniform but much smaller amplitude. This is probably
            a unique feature of 2D that does not generalize to higher
            dimensions.</p>
            <h2
            id="feature-geometry-a-lattice-on-hypersphere-surface">Feature
            geometry: a lattice on hypersphere surface</h2>
            <p>As we have mentioned previously, when the embedding is
            non-degenerate, the weight vectors of all embedded features
            tend to have approximately the same amplitude; in other
            words, for a n-D model they lay on the surface of a n-D
            hypersphere. We also know that within the same model, the
            distance to the nearest embedded feature is approximately
            the same for all embedded features (see <span
            class="math inline">\(\theta_{\rm min}\)</span> plot
            earlier), suggesting that they form an approximately uniform
            lattice. This could be a natural consequence of minimizing
            interference by maximizing the separation between nearby
            features, similar to a high-dimension version of the Thomson
            problem.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/theta_distribution.png"
            alt="theta_distribution.png" style="width: 400px"/>
            <figcaption
            aria-hidden="true">theta_distribution.png</figcaption>
            </figure>
            <p>The figure above shows the distribution of the separation
            between all pairs of weight vectors of embedded features for
            one of our models. Overall, it resembles the distribution
            for uniformly distributed directions on the n-D hypersphere
            surface (blue line). We can also see deviation from this
            uniform distribution due to the weight vectors developing a
            lattice pattern in attempt to maximize <span
            class="math inline">\(\theta_{\rm min}\)</span>.
            Specifically, all features that would be at a smaller theta
            are now packed near <span class="math inline">\(\theta_{\rm
            min}\)</span>, creating a peak there. We also see some
            additional, more diffuse peaks, and those likely correspond
            to second and third neighbors on the lattice (i.e., points
            with Manhattan distance 2 and 3).</p>
            <p>It is worth noting that the features do try to spread
            over the whole hypersphere surface, as opposed to
            self-organize into lower-dimensional objects. (If they did,
            the <span class="math inline">\(\theta\)</span> distribution
            would have a strong peak at <span
            class="math inline">\(\pi/2\)</span>.) This is in contrast
            to the low-sparsity behaviors observed in the Toy Model
            paper, where features tend to organize into low-dimensional
            polygons. This difference highlights the importance of
            extending our understanding of superposition into the sparse
            limit. This also calls for a more detailed investigation on
            how the preferred dimension of packing scales with sparsity
            and whether there is a phase transition from partitioning
            into smaller subspaces (e.g., low-dimensional polygons) and
            using all dimensions.</p>
            <h2
            id="model-response-trading-sensitivity-with-more-features">Model
            response: trading sensitivity with more features</h2>
            <p>Now let’s turn to the response pattern of the model. Here
            I mainly provide some qualitative intuitions; a more
            quantitative analysis is left to the theoretical framework
            later in this note.</p>
            <p>In the degenerate packing regime, the model response
            becomes weaker (reduced weight vector amplitude and model
            output) at increasing multiplicity. This result is easy to
            understand, as degenerate embedding at uniform amplitudes
            means that the model gives the same output in all degenerate
            dimensions; as multiplicity increases, the model favors
            weaker outputs for it generate less errors on the degenerate
            dimensions other than the input dimension.</p>
            <p>In the non-degenerate regime, we see a different trend:
            as <span class="math inline">\(n_{\rm feature}\)</span>
            increases, the response becomes steeper and the intercept on
            the input axis (i.e. the minimum input that is not fully
            suppressed by the bias) also increases. A good example is
            the response for 8D models below.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/intercept.png"
            alt="intercept.png" style="width: 800px"/>
            <figcaption aria-hidden="true">intercept.png</figcaption>
            </figure>
            <p>This behavior can be interpreted as a result of avoiding
            interference between adjacent features. For a nearly
            symmetric embedding (i.e., all features have similar
            amplitudes, biases, and <span
            class="math inline">\(\theta_{\rm min}\)</span>), an
            intercept of <span class="math inline">\(\cos\theta_{\rm
            min}\)</span> is sufficient to suppress all interference
            between adjacent features. In general, the nature of the L2
            error means that the optimize setup is always an intercept
            that is slightly smaller than <span
            class="math inline">\(\cos\theta_{\rm min}\)</span>;
            however, since the number of adjacent neighboring points on
            a lattice increases exponentially with lattice dimension, as
            <span class="math inline">\(n\)</span> increases the optimal
            intercept should quickly converge towards <span
            class="math inline">\(\cos\theta_{\rm min}.\)</span></p>
            <p>The notion of an intercept at <span
            class="math inline">\(\approx\cos\theta_{\rm min}\)</span>
            is consistent with our model results. Note that in 8D the
            intercept still lies below <span
            class="math inline">\(\cos\theta_{\rm min}\)</span> by a
            noticeable amount, because in this case the number of
            neighbors of each feature is not still too large -
            considering spherical packing in <span
            class="math inline">\(n-1=7\)</span> dimensions yields <span
            class="math inline">\(\sim 10^2\)</span> neighbors.</p>
            <p>One interesting implication is that <strong>in the
            non-degenerate regime and large <span
            class="math inline">\(n\)</span>, interference remains
            negligible no matter how many features are packed</strong>.
            New features are embedded only at the cost of ignoring weak
            inputs.</p>
            <h2
            id="inefficient-training-at-degenerate-packing">Inefficient
            training at degenerate packing</h2>
            <p>Another interesting observation is that in the degenerate
            regime the model often end up in a suboptimal local minimum,
            which suggests inefficient training.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/training_inefficiency.png"
            alt="training_inefficiency.png" style="width: 400px"/>
            <figcaption
            aria-hidden="true">training_inefficiency.png</figcaption>
            </figure>
            <p>The figure above shows the net gain of the models,
            defined as</p>
            <p><span class="math display">\[
            g\equiv (l_0-\bar{l})n_{\rm feature}.
            \]</span></p>
            <p>Here <span class="math inline">\(\bar{l}\)</span> is the
            average loss, and <span
            class="math inline">\(l_0=1/3\)</span> represents the loss
            of not embedding any feature. This gain function is designed
            so that it stays the same when we add unembedded new
            features. As a result, for optimal embedding, when <span
            class="math inline">\(n_{\rm feature}\)</span> increases,
            gain should be non-decreasing. This provides a sufficient
            (but not necessary) criterion of suboptimal embedding: if
            there exist any embedding with smaller <span
            class="math inline">\(n_{\rm feature}\)</span> and larger
            gain, the embedding has to be suboptimal. Following this
            criterion, we can see that the 3D and 4D models show clear
            sign of suboptimal embedding as soon as degeneracy develops.
            As a side note, the 2D model with 128 features, which shows
            a hexagon pattern qualitatively different from nearby
            models, is also suboptimal.</p>
            <p>The suboptimal results suggest that training may be
            inefficient in the degenerate regime. I conjecture that this
            could be associated with the existence of many local minima.
            The multiplicity of a degenerate feature is quantized (as it
            needs to be an integer), and for each multiplicity there
            could be a local minimum of the loss function. This web of
            local minimum could trap the model before it reaches the
            global minimum.</p>
            <h2
            id="changing-loss-function-can-suppress-degeneracy">Changing
            loss function can suppress degeneracy</h2>
            <p>Overall, degeneracy appears to be an undesirable behavior
            since it makes training less efficient and, in the limit of
            high degeneracy, leaves all features essentially useless
            because the response is always very weak. So, is there any
            way to avoid this behavior?</p>
            <p>Fundamentally, degeneracy arises because having multiple
            features at large error is still better than accurately
            embedding a single feature but ignoring the rest. This is
            related to the property of the L2 loss function, which
            provides a strong incentive to improve the least accurate
            results (the ignored features).</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/huber.png"
            alt="huber.png" style="width: 400px"/>
            <figcaption aria-hidden="true">huber.png</figcaption>
            </figure>
            <p>It is therefore reasonable to expect the behavior to
            change at a different loss function. In particular,
            degeneracy should reduce when we switch to a loss function
            with less prioritization on reducing large errors. In the
            figure above, I consider a Huber loss function with varying
            <span class="math inline">\(\delta\)</span>; this becomes a
            L1/L2 loss at <span
            class="math inline">\(\delta=0/1\)</span>. As <span
            class="math inline">\(\delta\)</span> decreases, there is a
            phase transition from degenerate packing of nearly all
            features to non-degenerate packing. Further reducing delta
            causes the model to prefer embedding fewer features.</p>
            <p>It would be useful to think a little more about how this
            relates to real-world applications. For many real-world
            applications, we can only tolerate error up to a certain
            amount; beyond that, the result is useless so it is as bad
            as (if not worse than) simply ignoring the input. For
            example, in networks that do multiple steps of computation,
            the amplification of error may effectively set an critical
            error for each stage beyond which the final result will be
            significantly corrupted. Therefore, if we want to use this
            kind of toy model as an abstraction of, say, one layer of a
            large model, it is beneficial to consider a loss function
            other than L2, or even a loss function that behaves like L2
            near zero but flattens out beyond a certain point (e.g.,
            1-Gaussian), and study how superposition is affected by the
            critical error of the loss function.</p>
            <h1
            id="a-theoretical-framework-of-sparse-feature-embedding-in-high-dimensions">A
            theoretical framework of sparse feature embedding in high
            dimensions</h1>
            <h2 id="assumptions">Assumptions</h2>
            <p>The results in the previous section motivates a simple
            model for sparse feature embedding. It is worth stressing
            that this model is more of a tool for building intuition, as
            opposed to a rigorous mathematical proof. The key assumption
            of this model is that features are packed in an
            approximately <strong>symmetric</strong> way; this means</p>
            <ul>
            <li>All embedded features have the same multiplicity;</li>
            <li>All embedded features have the same weight vector
            amplitude and bias;</li>
            <li>The directions of weight vectors (or degenerate groups
            of weight vectors) of embedded features form a uniform
            lattice, where the separation between adjacent directions,
            <span class="math inline">\(\theta_{\rm min}\)</span>, is
            approximately constant for all directions.</li>
            </ul>
            <p>These assumptions are all consistent with our model
            results at <span class="math inline">\(n&gt;2\)</span>. One
            caveat is that for multiplicity we do tend to see some
            finite spread within a given model; it is unclear whether
            this will lead to any significant problem of this model.</p>
            <p>I also take the approximation of large <span
            class="math inline">\(n\)</span> when necessary.
            Specifically, this means keeping only the leading order
            <span class="math inline">\(n\)</span> dependence and
            ignoring any perturbations that vanishes in the <span
            class="math inline">\(n\to\infty\)</span> limit.</p>
            <p>With these assumptions, let’s calculate the optimal gain
            functions when a model embeds <span
            class="math inline">\(n_{\rm embed}\)</span> features at
            degeneracy <span class="math inline">\(n_{\rm deg}\)</span>
            in <span class="math inline">\(n_{\rm dir}=n_{\rm
            embed}/n_{\rm deg}\)</span> directions.</p>
            <h2 id="lattice-embedding-theta_rm-min-scaling">Lattice
            embedding: <span class="math inline">\(\theta_{\rm
            min}\)</span> scaling</h2>
            <p>First let us work out the separation of the lattice,
            <span class="math inline">\(\theta_{\rm min}\)</span>, in a
            n-D model when the lattice consists o <span
            class="math inline">\(n_{\rm dir}\)</span> directions. This
            is equivalent to packing (n-1)-D hyperspheres with radius
            <span class="math inline">\(\approx\theta_{\rm
            min}/2\)</span> on the surface of a n-D hypersphere. In the
            limit of small <span class="math inline">\(\theta_{\rm
            min}\)</span>, this is equivalent to the classic problem of
            hyperspheres packing in Cartesian space at (n-1)-D. To
            lowest order in <span class="math inline">\(\theta_{\rm
            min}\)</span>, this gives</p>
            <p><span class="math display">\[
            n_{\rm dir}(\theta_{\rm min}/2)^{n-1}V_{n-1} = \delta_{n-1}
            S_{n-1}.
            \]</span></p>
            <p>Here <span class="math inline">\(V_n\)</span> is the
            volume of a unit n-hypersphere, <span
            class="math inline">\(S_n\)</span> is the surface area of a
            (n+1)-hypersphere, and <span
            class="math inline">\(\delta_n\)</span> is the efficiency of
            hypersphere packing in n-D.</p>
            <p>[A note on small-<span class="math inline">\(\theta_{\rm
            min}\)</span> approximation: Since the hypersphere surface
            is not exactly Cartesian across finite angular span, the
            above result could slightly underestimate <span
            class="math inline">\(\theta_{\rm min}\)</span>. One can
            also obtain an upper bound of theta by replacing <span
            class="math inline">\(\theta_{\rm min}/2\)</span> with <span
            class="math inline">\(2\sin(\theta_{\rm min}/4)\)</span>.
            These two bounds are at most 10% different.]</p>
            <p><span class="math inline">\(V_n\)</span> and <span
            class="math inline">\(S_n\)</span> have simple analytic
            forms; for <span class="math inline">\(\delta_n\)</span>,
            hypersphere packing in arbitrary dimension is generally an
            unsolved problems but there are (1) exact result for a
            number of dimensions (1~8 and 24), and (2) upper and lower
            boundaries in large-n limit which constrains delta to ~
            factor of 2. (See <a
            href="https://mathworld.wolfram.com/HyperspherePacking.html">this
            page</a> and references therein.) Specifically, delta can be
            described using the <a
            href="https://mathworld.wolfram.com/HermiteConstants.html">Hermite
            constants</a> <span class="math inline">\(\gamma_n\)</span>,
            which are defined by</p>
            <p><span class="math display">\[
            \gamma_n = 4\left(\frac{\delta_n}{V_n}\right)^{2/n}
            \]</span></p>
            <p>A simple estimate is that <span
            class="math inline">\(\gamma_n/n\sim1/2\pi e\)</span>, with
            the prefactor on the RHS between 1 and 1.744 at large <span
            class="math inline">\(n\)</span>. Using this together with
            the formula for hypersphere surface area</p>
            <p><span class="math display">\[
            S_{n-1}=\frac{2\pi^{n/2}}{\Gamma(n/2)},
            \]</span></p>
            <p>and adopting Stirling’s approximation, we get the
            following relation:</p>
            <p><span class="math display">\[
            \frac{n_{\rm
            dir}}{\sqrt{2}}\sim\left(\frac{\sqrt{\xi}}{\theta_{\rm
            min}}\right)^{n-1},
            \]</span></p>
            <p>where <span class="math inline">\(\xi\)</span> is an
            order-unity parameter defined by</p>
            <p><span class="math display">\[
            \xi\equiv \frac{2\pi e\gamma_{n-1}}{n-1} \in [1,1.744]
            ~~{\rm at~large}~n.
            \]</span></p>
            <p>This result gives some very interesting implications:</p>
            <ul>
            <li>At a given <span class="math inline">\(\theta_{\rm
            min}\)</span>, as model dimension increases, the number of
            features that can be packed increases exponentially; the
            rate of this exponential increase depends on <span
            class="math inline">\(\theta_{\rm min}/\sqrt{\xi}\)</span>,
            which in practice may be related to the error tolerance of
            the problem.</li>
            <li>For large <span class="math inline">\(n\)</span>, as
            long as <span class="math inline">\(n_{\rm dir}\)</span> is
            below some exponentially large number, <span
            class="math inline">\(\theta_{\rm min}\)</span> is not very
            sensitive to <span class="math inline">\(n_{\rm
            dir}\)</span> and stays around <span
            class="math inline">\(\sqrt{\xi}\)</span>. This means:</li>
            <li>As long as <span class="math inline">\(n_{\rm
            feature}\)</span> is below some exponentially large number,
            the model can non-degenerately embed all features at
            reasonable accuracy (i.e., order-unity gain per feature)
            even when there are many more features than model
            dimensions.</li>
            <li>As long as <span class="math inline">\(n_{\rm
            feature}\)</span> is below some exponentially large number,
            adding new feature barely affects <span
            class="math inline">\(\theta_{\rm min}\)</span>; this may
            explain the tendency for models to pack more features (see
            following section).</li>
            <li>There may a very hard limit on much sensitivity a model
            in superposition can have: input below <span
            class="math inline">\(\cos\theta_{\rm min}\)</span> will
            generally by be ignored, whereas <span
            class="math inline">\(\cos\theta_{\rm
            min}&gt;\cos\sqrt{\xi}&gt;\cos\sqrt{1.744}=0.248\)</span>.
            (Note that this number is just a rough approximation due to
            the small-<span class="math inline">\(\theta_{\rm
            min}\)</span> approximation adopted earlier.)</li>
            </ul>
            <h2
            id="loss-function-why-packing-all-features-when-is-degeneracy-preferred">Loss
            function: Why packing all features? When is degeneracy
            preferred?</h2>
            <p>Now let’s quantitatively model <span
            class="math inline">\(W, b\)</span>, and the loss function.
            For a given feature (and all its degenerate peers), the
            output of the model is described with a ReLU-like profile
            with intercept <span
            class="math inline">\(-b_i/|W_i|^2\)</span> and slope <span
            class="math inline">\(k=|W_i|^2\)</span>. Since there are
            exponentially many neighbors at large <span
            class="math inline">\(n\)</span>, the optimal intercept
            asymptotes to <span class="math inline">\(\cos\theta_{\rm
            min}\)</span> - i.e., no interference between features.</p>
            <p>Minimizing the loss function at given <span
            class="math inline">\(n_{\rm deg}\)</span> and <span
            class="math inline">\(\theta_{\rm min}\)</span> gives</p>
            <p><span class="math display">\[
            k=\frac{1}{1+n_{\rm deg}}\frac{1+(\cos\theta_{\rm
            min})/2}{1-\cos\theta_{\rm min}},
            \]</span></p>
            <p>When the input is in an embedded direction, the average
            loss function is given by</p>
            <p><span class="math display">\[
            \bar l_{\rm embed} = \frac{1}{12}\frac{1}{n_{\rm
            deg}+1}(4n_{\rm deg}+3\cos^2\theta_{\rm
            min}+\cos^3\theta_{\rm min}).
            \]</span></p>
            <p>An the total gain is</p>
            <p><span class="math display">\[
            g\equiv(l_0-\bar l)n_{\rm feature}=(l_0-\bar l_{\rm
            embed})n_{\rm embed}\\=\frac{1}{12}\frac{n_{\rm
            embed}}{n_{\rm deg}+1}(4-3\cos^2\theta_{\rm
            min}-\cos^3\theta_{\rm min}).
            \]</span></p>
            <p>The figure below shows the gain for various <span
            class="math inline">\(n_{\rm embed}, n_{\rm deg}\)</span>
            and <span class="math inline">\(n\)</span>. The location
            where <span class="math inline">\(n_{\rm deg}=1\)</span>
            becomes less optimal than <span class="math inline">\(n_{\rm
            deg}=2\)</span> (marked by red vertical line) serves as a
            good estimate of the phase transition to degeneracy in our
            toy models. We can also see that the <span
            class="math inline">\(n=2,3\)</span> results seem
            qualitatively different from the higher <span
            class="math inline">\(n\)</span> results, which serves as a
            caution that insights from toy models at very low dimension
            may not always generalize into higher dimensions.</p>
            <figure>
            <img
            src="Superposition%20of%20sparse%20features%20Exponential%20embed%201f3831dee5098038bdddff7cb4987d36/analytic_gain.png"
            alt="analytic_gain.png" style="width: 800px"/>
            <figcaption
            aria-hidden="true">analytic_gain.png</figcaption>
            </figure>
            <p>For large <span class="math inline">\(n\)</span> and
            <span class="math inline">\(n_{\rm feature}\)</span> values
            not directly accessible in our models, analyzing the
            derivatives of <span class="math inline">\(g\)</span> helps
            us understand how the model determines whether to embed more
            features and whether it does so in degeneracy. Our previous
            analysis on <span class="math inline">\(\theta_{\rm
            min}\)</span> gives</p>
            <p><span class="math display">\[
            \frac{\partial\log\theta_{\rm min}(n_{\rm embed}, n_{\rm
            deg})}{\partial\log n_{\rm embed}} =
            -\frac{1}{n-1},~~~\frac{\partial\log\theta_{\rm min}(n_{\rm
            embed}, n_{\rm deg})}{\partial\log n_{\rm deg}} =
            \frac{1}{n-1}.
            \]</span></p>
            <p>To lowest nontrivial order in <span
            class="math inline">\(\theta_{\rm min}\)</span>, we have</p>
            <p><span class="math display">\[
            \frac{\partial g(n_{\rm embed}, n_{\rm deg})}{\partial
            n_{\rm embed}} \approx \frac{9}{24}\frac{1}{n_{\rm
            deg}+1}\left(1-\frac{2}{n-1}\right)\theta_{\rm min}^2.
            \]</span></p>
            <p>We can immediately see why <span
            class="math inline">\(n=2,3\)</span> are different: these
            are the only cases where <span
            class="math inline">\(g\)</span> does not always increase
            with <span class="math inline">\(n_{\rm embed}\)</span>.
            <strong>For models with <span
            class="math inline">\(n&gt;3\)</span>, it is always
            advantageous to embed more features.</strong></p>
            <p>Meanwhile, the derivative with respect to multiplicity
            is</p>
            <p><span class="math display">\[
            \frac{\partial g(n_{\rm embed}, n_{\rm deg})}{\partial
            n_{\rm deg}} \approx \frac{9}{24}\frac{n_{\rm embed}}{n_{\rm
            deg}+1}\left(\frac{2}{n_{\rm deg}(n-1)}-\frac{1}{n_{\rm
            deg}+1}\right)\theta_{\rm min}^2.
            \]</span></p>
            <p>For <span class="math inline">\(n&gt;5\)</span>, <span
            class="math inline">\(\partial g/\partial n_{\rm
            deg}&lt;0\)</span> for all <span
            class="math inline">\(n_{\rm deg}\geq 1\)</span>. In other
            words, <strong>models with <span
            class="math inline">\(n&gt;5\)</span> always prefer
            non-degenerate packing over degenerate packing</strong>.
            This is consistent with our numerical results, though one
            may need to survey much larger <span
            class="math inline">\(n_{\rm feature}\)</span> values before
            using that as a strong empirical evidence for this.</p>
            <h1 id="questions-for-future-studies">Questions for future
            studies</h1>
            <h2
            id="probing-the-boundaries-of-hyperspherical-geometry">Probing
            the boundaries of hyperspherical geometry</h2>
            <p>The main finding in this study is that features in a n-D
            model tend to pack into a near-uniform lattice on the
            surface of the n-D hypersphere; for large n and L2 loss
            function, the model always embed all features and always do
            so without degeneracy.</p>
            <p>To understand whether this regime is relevant to
            real-world applications, it is important to understand how
            these results generalize to finite sparsity and different
            loss functions:</p>
            <p><strong>Finite sparsity:</strong> In real-world examples,
            we roughly expect <span class="math inline">\((1-S)n_{\rm
            feature}\)</span> to be <span class="math inline">\(\gtrsim
            1\)</span> but still <span class="math inline">\(\ll
            n\)</span>. Does this affect the behavior? Intuitively,
            having <span class="math inline">\(\mathcal{O}(1)\)</span>
            coexisting features may be fine since they are unlikely to
            be neighbors in the embedding. But accurately determining
            the condition when the spare assumption breaks down requires
            future studies.</p>
            <p>Previous studies on compressed sensing (e.g., <a
            href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611973075.95">Ba
            et al. 2010</a>) could also be quite relevant to this topic,
            though one key difference is that their problem requires
            minimizing the maximum error whereas for typical LLM
            applications we care more about mean error; having very few
            (perhaps exponentially few) outliers is usually acceptable.
            This may lead to different optimal configurations.</p>
            <p><strong>Different loss functions:</strong> As discussed
            earlier, in some real-world applications we can only
            tolerate a finite amount of error (e.g., when this toy model
            just represents one layer of a large model). How does the
            embedding dynamics change in that case? My conjecture is
            that setting a characteristic error tolerance limits the
            minimum <span class="math inline">\(\theta_{\rm
            min}\)</span> below which the model stops embedding
            additional features.</p>
            <h2
            id="how-are-feature-importancecorrelationsoperations-embedded-in-high-dimensions">How
            are feature importance/correlations/operations embedded in
            high dimensions?</h2>
            <p>The original Toy Model paper touched on the subject of
            feature importance and correlation shaping the geometry of
            embedding. One natural questions to ask is to what extent
            these results generalize to higher model dimensions and
            sparse features.</p>
            <p>A key difference between high and low <span
            class="math inline">\(n\)</span> is that in high dimensions
            the interference between features is much weaker; therefore,
            it may be much easier to handle correlated features.</p>
            <p>On the other hand, it is no longer clear whether feature
            importance could cause it to occupy a larger volume. At high
            <span class="math inline">\(n\)</span>, since the volume of
            the (n-1)-D hypersphere taken up by each feature increases
            rapidly with <span class="math inline">\(\theta_{\rm
            min}\)</span>, it may become uneconomic to increase <span
            class="math inline">\(\theta_{\rm min}\)</span> for more
            important features.</p>
            <p>Another direction that requires better understanding is
            how operations are embedded in superposition. The Toy Model
            paper touched on this subject by considering abs() as an
            example of a nonlinear function. However, the limitation of
            this choice is that it is a in-place operation whereas in
            reality features interact a lot with each other; to some
            extent we can view the features as functions and the model
            is just doing a reduction in lambda calculus. So, how does
            the geometrical structure of the features, as described by
            how they interact with each other, affect the embedding? I
            find this to be a very interesting direction, though
            designing an appropriate test problem could be
            challenging.</p>
            <h2
            id="real-world-implications-of-hyperspherical-geometry">Real-world
            implications of hyperspherical geometry</h2>
            <p><strong>Do we see spherical geometry in models for
            real-world applications?</strong> Perhaps the simplest test
            to understand whether our intuition from the toy model is
            relevant is to check how models trained for realistic tasks
            embed features and whether that resemble the hypersphere
            layout in the toy models. For example, one may take a
            feature extraction result and map the features back to the
            original network, and study the distribution of the angular
            separation between these features; do they show the
            near-uniform lattice we saw for toy models?</p>
            <p><strong>Feature extraction:</strong> This work suggests
            that an efficient model would use all its dimensions for all
            the features; there is generally no low-dimensional
            subspaces that embed subsets of features. This sounds like
            bad news for feature extraction, as it means that feature
            extraction can only be done globally; for example, one
            probably cannot use a subset of a layer to perform feature
            extraction. On the other hand, one interesting possibility
            is that the this spherical embedding may also be somewhat
            holographic, in that a subset of the dimensions may already
            noisily encode the whole embedding. If this is the case, it
            may have some interesting implications for efficient feature
            extraction and model distillation.</p>
            <p><strong>Model robustness:</strong> Similar to the
            discussion above, the spherical embedding offers two
            opposite intuitions. On the one hand, since superposition
            makes use of all dimensions simultaneously, no part is left
            untouched if we perturb one of the dimensions (either in
            input or in the model). On the other hand, since all
            dimensions are used, the effect of perturbing - or even
            eliminating - one of the dimensions may be quite limited.
            Which of these are more correct? The fact that the human
            brain works quite well despite neurons randomly dying or
            malfunctioning may suggest a good amount of robustness, and
            it will be fun to demonstrate similar robustness in an
            artificial neural network with strong superposition.</p>
            <p><strong>Scaling laws:</strong> The number of sparse
            features that can be embedded with a given accuracy
            requirement scales exponentially with model size. But why do
            real-world model performance tend to show more power-law
            scalings? Does this just imply that superposition is not the
            bottleneck of model performance for any sufficiently large
            model, or are there additional considerations (e.g., error
            tolerance, feature interactions) that eventually reduce this
            exponential scaling to a power-law?</p>
        </div>
    </main>

    <div data-include="/footer.html"></div>
</body>
</html>